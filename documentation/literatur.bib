@misc{tiger_laub, 
   author       = {Welt.de}, 
   title        = {{Tiger im Laub}},   
	 note					= {\url{https://img.welt.de/img/kmpkt/mobile202692882/4822508647-ci102l-w1024/GettyImages-144104549-1.jpg}}
 }

@misc{tiger_stein, 
   author       = {zoovienna.at}, 
   title        = {{Tiger liegt auf einem Stein}},  
	 note					= {\url{https://www.zoovienna.at/media/_versions_/hotnews/pa_1_ina_animal_detail_801.jpg}}
 }

@misc{tiger_leicht_verdeckt, 
   author       = {wixmp.com}, 
   title        = {{Tiger ist leicht verdeckt}},  
	 note					= {\href{https://images-wixmp-ed30a86b8c4ca887773594c2.wixmp.com/f/135b05ab-e0e2-44a4-9667-eb0454b5589e/d5dps9i-7c375cfd-b291-40fb-8807-7c41a5efa272.jpg?token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJ1cm46YXBwOjdlMGQxODg5ODIyNjQzNzNhNWYwZDQxNWVhMGQyNmUwIiwiaXNzIjoidXJuOmFwcDo3ZTBkMTg4OTgyMjY0MzczYTVmMGQ0MTVlYTBkMjZlMCIsIm9iaiI6W1t7InBhdGgiOiJcL2ZcLzEzNWIwNWFiLWUwZTItNDRhNC05NjY3LWViMDQ1NGI1NTg5ZVwvZDVkcHM5aS03YzM3NWNmZC1iMjkxLTQwZmItODgwNy03YzQxYTVlZmEyNzIuanBnIn1dXSwiYXVkIjpbInVybjpzZXJ2aWNlOmZpbGUuZG93bmxvYWQiXX0.r3N27iKVOpqX5ipBI7ZOQ8-joomhq6Jjbti5E7mBl_s}
	{
	\\
	\nolinkurl{https://images-wixmp-ed30a86b8c4ca887773594c2.wixmp.com/}
	\\
	\nolinkurl{f/135b05ab-e0e2-44a4-9667-eb0454b5589e/d5dps9i-7c375cfd-b291-}
	\\
	\nolinkurl{40fb-8807-7c41a5efa272.jpg?token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1N}
	\\
	\nolinkurl{iJ9.eyJzdWIiOiJ1cm46YXBwOjdlMGQxODg5ODIyNjQzNzNhNWYwZDQxNWVhMGQyNmUwIiwia}
	\\
	\nolinkurl{XNzIjoidXJuOmFwcDo3ZTBkMTg4OTgyMjY0MzczYTVmMGQ0MTVlYTBkMjZlMCIsIm9iai}
	\\
	\nolinkurl{I6W1t7InBhdGgiOiJcL2ZcLzEzNWIwNWFiLWUwZTItNDRhNC05NjY3LWViMDQ1NGI1NTg}
	\\
	\nolinkurl{5ZVwvZDVkcHM5aS03YzM3NWNmZC1iMjkxLTQwZmItODgwNy03YzQxYTVlZmEyNzIuanBn}
	\\
	\nolinkurl{In1dXSwiYXVkIjpbInVybjpzZXJ2aWNlOmZpbGUuZG93bmxvYWQiXX0.r3N27iKVOpqX5}
	\\
	\nolinkurl{ipBI7ZOQ8-joomhq6Jjbti5E7mBl_s}
	}
	}
}

@misc{tiger_verdeckt, 
   author       = {Welt.de}, 
   title        = {{Tiger versteckt im Gras}},   
	 note					= {\url{https://www.welt.de/img/vermischtes/mobile112538261/6561624327-ci23x11-w1920/African-Scops-Owl-Otus-senegalensis-camouflaged-in-a-tree-Etosha-National-4.jpg}}
 }

%%%%%%%%%%%%%%%%%%% Paper %%%%%%%%%%%%%%%%%%%

@INPROCEEDINGS{DCT_Interpolation:2017,
  author		={A. M. {Abdelsalam} and J. M. P. {Langlois} and F. {Cheriet}},
  booktitle	={2017 IEEE 25th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM)}, 
  title			={A Configurable FPGA Implementation of the Tanh Function Using DCT Interpolation}, 
  year			={2017},
  volume		={},
  number		={},
  pages			={168-171},
}

@inproceedings{going_deeper_embedded_FPGA:2016,
	author = {Qiu, Jiantao and Wang, Jie and Yao, Song and Guo, Kaiyuan and Li, Boxun and Zhou, Erjin and Yu, Jincheng and Tang, Tianqi and Xu, Ningyi and Song, Sen and Wang, Yu and Yang, Huazhong},
	title = {Going Deeper with Embedded FPGA Platform for Convolutional Neural Network},
	year = {2016},
	isbn = {9781450338561},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/2847263.2847265},
	doi = {10.1145/2847263.2847265},
	abstract = {In recent years, convolutional neural network (CNN) based methods have achieved great success in a large number of applications and have been among the most powerful and widely used techniques in computer vision. However, CNN-based methods are com-putational-intensive and resource-consuming, and thus are hard to be integrated into embedded systems such as smart phones, smart glasses, and robots. FPGA is one of the most promising platforms for accelerating CNN, but the limited bandwidth and on-chip memory size limit the performance of FPGA accelerator for CNN.In this paper, we go deeper with the embedded FPGA platform on accelerating CNNs and propose a CNN accelerator design on embedded FPGA for Image-Net large-scale image classification. We first present an in-depth analysis of state-of-the-art CNN models and show that Convolutional layers are computational-centric and Fully-Connected layers are memory-centric.Then the dynamic-precision data quantization method and a convolver design that is efficient for all layer types in CNN are proposed to improve the bandwidth and resource utilization. Results show that only 0.4% accuracy loss is introduced by our data quantization flow for the very deep VGG16 model when 8/4-bit quantization is used. A data arrangement method is proposed to further ensure a high utilization of the external memory bandwidth. Finally, a state-of-the-art CNN, VGG16-SVD, is implemented on an embedded FPGA platform as a case study. VGG16-SVD is the largest and most accurate network that has been implemented on FPGA end-to-end so far. The system on Xilinx Zynq ZC706 board achieves a frame rate at 4.45 fps with the top-5 accuracy of 86.66% using 16-bit quantization. The average performance of convolutional layers and the full CNN is 187.8 GOP/s and 137.0 GOP/s under 150MHz working frequency, which outperform previous approaches significantly.},
	booktitle = {Proceedings of the 2016 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
	pages = {26â€“35},
	numpages = {10},
	keywords = {convolutional neural network (cnn), bandwidth utilization, dynamic-precision data quantization, embedded fpga},
	location = {Monterey, California, USA},
	series = {FPGA '16}
}

@article{speeding_up_conv_network:2014,
   title			= {Speeding up Convolutional Neural Networks with Low Rank Expansions},
   ISBN				= {1901725529},
   url				= {http://dx.doi.org/10.5244/C.28.88},
   DOI				= {10.5244/c.28.88},
   journal		= {Proceedings of the British Machine Vision Conference 2014},
   publisher	= {British Machine Vision Association},
   author			= {Jaderberg, Max and Vedaldi, Andrea and Zisserman, Andrew},
   year				= {2014}
}

@inproceedings{improving_neuronal_net_speed_cpu:2011,
	title 		= {Improving the speed of neural networks on CPUs},
	author  	= {Vincent Vanhoucke and Andrew Senior and Mark Z. Mao},
	year  		= {2011},
	booktitle = {Deep Learning and Unsupervised Feature Learning Workshop, NIPS 2011}
}

@misc{caffe_framework,
	author		= {Jia, Yangqing and Shelhamer, Evan},
	title			= {Caffe},
	url				= {http://caffe.berkeleyvision.org/}
}

@book{ki_uebungsbuch:2001,
  title			={Lehr- und {\"U}bungsbuch k{\"u}nstliche Intelligenz: mit 48 Tabellen},
  author		={L{\"a}mmel, U. and Cleve, J.},
  isbn			={9783446214217},
  url				={https://books.google.de/books?id=hrlHAAAACAAJ},
  year			={2001},
  publisher	={Fachbuchverl. Leipzig im Carl-Hanser-Verlag}
}

@book{opencl_programmer_guide:2011,
  title			={OpenCL Programming Guide (OpenGL) },
  author		={Munshi, Aaftab},
  isbn			={978-0321749642},
  url				={https://www.amazon.de/OpenCL-Programming-OpenGL-Aaftab-Munshi/dp/0321749642},
  year			={2011},
  publisher	={Addison-Wesley Professional}
}

@article{parallelization_neuronal_net:2013,
title 		= "Multicore and GPU Parallelization of Neural Networks for Face Recognition",
journal 	= "Procedia Computer Science",
volume 		= "18",
pages 		= "349 - 358",
year 			= "2013",
note 			= "2013 International Conference on Computational Science",
issn 			= "1877-0509",
doi	 			= "https://doi.org/10.1016/j.procs.2013.05.198",
url 			= "http://www.sciencedirect.com/science/article/pii/S1877050913003414",
author 		= "Altaf Ahmad Huqqani and Erich Schikuta and Sicen Ye and Peng Chen",
keywords 	= "Parallel Simulation, Multicores, GPUs, OpenMP, CUDA, Artificial Neural Network",
abstract 	= "Training of Artificial Neural Networks for large data sets is a time consuming task. Various approaches have been proposed to reduce the efforts, many of them by applying parallelization techniques. In this paper we develop and analyze two novel parallel training approaches for Backpropagation neural networks for face recognition. We focus on two specific paralleliza- tion environments, using on the one hand OpenMP on a conventional multithreaded CPU and CUDA on a GPU. Based on our findings we give guidelines for the efficient parallelization of Backpropagation neural networks on multicore and GPU architectures. Additionally, we present a traversal method finding the best combination of learning rate and momentum term by varying the number of hidden neurons supporting the parallelization efforts."
}